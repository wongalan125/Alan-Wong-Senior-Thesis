{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import textblob\n",
    "import requests\n",
    "import re\n",
    "import math\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= 'XXX'\n",
    "consumer_secret= 'XXX'\n",
    "access_token= 'XXX'\n",
    "access_token_secret= 'XXX'\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortDictByKey(dict, ascending = True):\n",
    "    sorted_dict = {}\n",
    "    sorted_keys = sorted(dict.keys(), reverse=not ascending)\n",
    "    for i in sorted_keys:\n",
    "        sorted_dict[i] = dict[i]\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the names of text files to be hydrated\n",
    "def getTextFileNames(reverse=False):\n",
    "    file_names = []\n",
    "    for path in Path().iterdir():\n",
    "        if path.name.endswith('.txt'):\n",
    "            file_names.append(path.name)\n",
    "    file_names.sort(reverse=reverse)\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes Twitter account tokens as input and returns API object\n",
    "def getTwitterAPIObject(consumer_key, consumer_secret, access_token, access_token_secret):\n",
    "    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api_keys = tw.API(auth, wait_on_rate_limit=True)\n",
    "    return api_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a list of Tweet IDs and hydrates them\n",
    "def hydrateListOfIDs(api_keys, ids, output_name, traceRootTweet=False):\n",
    "    # hydrate Tweets in groups of 100\n",
    "    exit = False # exit loop if error occurs\n",
    "    with jsonlines.open(output_name, mode='w') as writer:\n",
    "        for i in range(len(ids)//100+1):\n",
    "            ids_set = ids[100*i:100+100*i] # for regular sets\n",
    "            if i == len(ids)//100: # for the last set\n",
    "                ids_set = ids[100*i:len(ids)]\n",
    "            if (len(ids_set) == 0):\n",
    "                continue\n",
    "            try: \n",
    "                statuses = api_keys.statuses_lookup(ids_set, tweet_mode=\"extended\")\n",
    "            except:\n",
    "                print(\"Error occured\")\n",
    "                Path(output_name).unlink()\n",
    "                print(\"Deleted:\",output_name)\n",
    "                exit = True\n",
    "                break\n",
    "            # write to jsonl file\n",
    "            if (traceRootTweet):\n",
    "                for status in statuses:\n",
    "                    try:\n",
    "                        OG_id = status._json['retweeted_status']['id']\n",
    "                        status = api_keys.statuses_lookup([OG_id], tweet_mode=\"extended\")[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    writer.write(status._json)\n",
    "            else:\n",
    "                for status in statuses:\n",
    "                    writer.write(status._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a list of text files containing Tweet IDs and hydrates them\n",
    "def hydrateListOfFiles(api_keys,api_name, file_names):\n",
    "    # track hydration success\n",
    "    try:\n",
    "        stats_file = jsonlines.open(api_name + \" Hydration Success.jsonl\",)\n",
    "        for line in stats_file:\n",
    "            hydration_success = line\n",
    "    except:\n",
    "        hydration_success = {}\n",
    "    # hydrate each file\n",
    "    for file_name in file_names:\n",
    "        time_start = time.perf_counter()\n",
    "        file = open(file_name, \"r\")\n",
    "        output_name = file_name[:-4] + \".jsonl\"\n",
    "        if Path(output_name).is_file():\n",
    "            continue\n",
    "        print(file_name)\n",
    "        # read in Tweet IDs\n",
    "        ids = []\n",
    "        for line in file:\n",
    "            ids.append(int(line))\n",
    "        # hydrate Tweets in groups of 100\n",
    "        count = 0\n",
    "        exit = False # triggers to True if error occurs\n",
    "        with jsonlines.open(output_name, mode='w') as writer:\n",
    "            for i in range(len(ids)//100+1):\n",
    "                ids_set = ids[100*i:100+100*i] # for regular sets\n",
    "                if i == len(ids)//100: # for the last set\n",
    "                    ids_set = ids[100*i:len(ids)]\n",
    "                if (len(ids_set) == 0):\n",
    "                    continue\n",
    "                try: \n",
    "                    statuses = api_keys.statuses_lookup(ids_set, tweet_mode=\"extended\")\n",
    "                except:\n",
    "                    print(\"Timed out\")\n",
    "                    Path(output_name).unlink()\n",
    "                    print(\"Deleted:\",output_name)\n",
    "                    exit = True\n",
    "                    break\n",
    "                # write to jsonl file\n",
    "                for status in statuses:\n",
    "                    count += 1\n",
    "                    writer.write(status._json)\n",
    "        time_end = time.perf_counter()\n",
    "        duration = str(round((time_end-time_start)/60,2)) + \" mins\"\n",
    "        hydration_success[file_name] = [count, len(ids), duration]\n",
    "        hydration_success = sortDictByKey(hydration_success)\n",
    "        if exit:\n",
    "            with jsonlines.open(api_name + \" Hydration Success.jsonl\", mode='w') as writer:\n",
    "                writer.write(hydration_success)\n",
    "            break\n",
    "    with jsonlines.open(api_name + \" Hydration Success.jsonl\", mode='w') as writer:\n",
    "        writer.write(hydration_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a list of JSONL dictionaries and combines them into one dictionary\n",
    "def combineJsonlDicts(jsonl_names_list):\n",
    "    dicts = []\n",
    "    for i in range(len(jsonl_names_list)):\n",
    "        dicts.append(jsonlines.open(jsonl_names_list[i],))\n",
    "    for i in range(len(dicts)):\n",
    "        for line in dicts[i]:\n",
    "            if (i == 0):\n",
    "                new_dict = line\n",
    "            else:\n",
    "                new_dict.update(line)\n",
    "    new_dict = sortDictByKey(new_dict)\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions on individual tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the full text of a Tweet\n",
    "def getOriginalText(status):\n",
    "    try:\n",
    "        return status['retweeted_status']['full_text']\n",
    "    except:\n",
    "        try:\n",
    "            return status['full_text']\n",
    "        except:\n",
    "            return status['text']\n",
    "# checks to see if a Tweet is a retweet\n",
    "def isRetweet(status):\n",
    "    try:\n",
    "        test = status['retweeted_status']\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "# returns the status that is being replied to, if it exists\n",
    "def getReplyID(status):\n",
    "    if (status['in_reply_to_status_id'] == None):\n",
    "        return 9999999999999999999\n",
    "    else:\n",
    "        return status['in_reply_to_status_id']\n",
    "# returns the user that is being replied to, if they exist\n",
    "def getReplyUser(status):\n",
    "    if (status['in_reply_to_user_id'] == None):\n",
    "        return 9999999999999999999\n",
    "    else:\n",
    "        return status['in_reply_to_user_id']\n",
    "# gets the Tweet ID of the status being retweeted    \n",
    "def getOriginalTweetID(status):\n",
    "    try:\n",
    "        return status['retweeted_status']['id']\n",
    "    except: \n",
    "        return 9999999999999999999\n",
    "# formats datetime object\n",
    "def formatDateTime(dtime):\n",
    "    return datetime.strftime(datetime.strptime(dtime,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "# gets datetime object of original tweet\n",
    "def getOGDateTime(status):\n",
    "    try:\n",
    "        return formatDateTime(status['retweeted_status']['created_at'])\n",
    "    except:\n",
    "        return None\n",
    "# gets the sensitivity level of a status\n",
    "def getSensitivity(status):\n",
    "    try:\n",
    "        return s['possibly_sensitive']\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "def getTweetInfo(s):\n",
    "    tweetID = s['id']\n",
    "    date = formatDateTime(s['created_at'])\n",
    "    text = getOriginalText(s)\n",
    "    isRT = isRetweet(s)\n",
    "    numRetweets = s['retweet_count']\n",
    "    numFavorites = s['favorite_count']\n",
    "    user_name = s['user']['screen_name']\n",
    "    user_date = s['user']['created_at']\n",
    "    numFollowers = s['user']['followers_count']\n",
    "    numFriends = s['user']['friends_count']\n",
    "    verified = s['user']['verified']\n",
    "    location = s['user']['location']\n",
    "    htags = s['entities']['hashtags']\n",
    "    mentions = s['entities']['user_mentions']\n",
    "    URLs = s['entities']['urls']\n",
    "    try:\n",
    "        media = s['entities']['media']\n",
    "    except: \n",
    "        media = None\n",
    "    replyID = getReplyID(s)\n",
    "    replyUser = getReplyUser(s)\n",
    "    replyName = s['in_reply_to_screen_name']\n",
    "    sens = getSensitivity(s)\n",
    "    lang = s['lang']\n",
    "    OGtweetID = getOriginalTweetID(s)\n",
    "    OGdate = getOGDateTime(s)\n",
    "    return [tweetID, date, text, isRT, numRetweets, numFavorites, user_name, user_date, numFollowers, numFriends, verified, location, htags, mentions, URLs, media, replyID, replyUser, replyName, sens, lang, OGtweetID, OGdate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing JSONLs into DFs, Pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get JSONL file names corresponding to a specific day\n",
    "def getFileNames(day, month):\n",
    "    start = 'coronavirus-tweet-id-2021-' + \"%02d\" % month + '-' \"%02d\" % day + '-00'\n",
    "    end = 'coronavirus-tweet-id-2021-' + \"%02d\" % month + '-' \"%02d\" % (day+1) + '-00'\n",
    "    file_names = []\n",
    "    for path in Path().iterdir():\n",
    "        if (path.name.startswith('corona') and path.name >= start and path.name < end and path.name.endswith('.jsonl')):\n",
    "            file_names.append(path.name)\n",
    "    file_names.sort()\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a list of JSONL files and returns a list with information about the statuses\n",
    "def jsonlToList(file_names, suppress=True):\n",
    "    time_start = time.perf_counter()\n",
    "    data = []\n",
    "    for file_name in file_names:\n",
    "        if (suppress == False):\n",
    "            print(file_name)\n",
    "        f = jsonlines.open(file_name,)\n",
    "        for status in f.iter():\n",
    "            tweetInfo = getTweetInfo(status)\n",
    "            data.append(tweetInfo)\n",
    "    time_end = time.perf_counter()\n",
    "    duration = str(round((time_end-time_start)/60,2)) + \" mins\"\n",
    "    if (suppress == False):\n",
    "        print(\"Read time:\",duration)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the list into a dataframe\n",
    "def listToDF(data, suppress=True, truth_labels=False):\n",
    "    time_start = time.perf_counter()\n",
    "    if truth_labels:\n",
    "        df = pd.DataFrame(data, columns = ['veracity', 'index','tweetID', 'date', 'text', 'isRT', '#Retweets','#Favorites', 'user_name', 'user_date', '#Followers', '#Friends', 'verified', 'location', 'hashtags', 'mentions','URLs', 'media', 'replyID', 'replyUser', 'replyName','sens', 'lang', 'OGtweetID', 'OGdate'])\n",
    "    else:\n",
    "        df = pd.DataFrame(data, columns = ['tweetID', 'date', 'text', 'isRT', '#Retweets','#Favorites', 'user_name', 'user_date', '#Followers', '#Friends', 'verified', 'location', 'hashtags', 'mentions','URLs', 'media', 'replyID', 'replyUser', 'replyName','sens', 'lang', 'OGtweetID', 'OGdate'])\n",
    "    time_end = time.perf_counter()\n",
    "    duration = str(round((time_end-time_start)/60,2)) + \" mins\"\n",
    "    if (suppress == False):\n",
    "        print(\"DF creation time:\",duration)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports a dataframe to a pickle\n",
    "def exportDFToPickle(day, month, df, suppress=True):\n",
    "    time_start = time.perf_counter()\n",
    "    pklname = \"%02d\" % month + '_' + \"%02d\" % day + '.pkl'\n",
    "    df.to_pickle(pklname)\n",
    "    time_end = time.perf_counter()\n",
    "    duration = str(round((time_end-time_start)/60,2)) + \" mins\"\n",
    "    if (suppress == False):\n",
    "        print(\"Pickle export time:\",duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortcut function to convert JSONLs directly to pickles\n",
    "def jsonlToPickle(startDay, endDay, month):\n",
    "    for day in range(startDay,endDay):\n",
    "        file_names = getFileNames(day, month)\n",
    "        data = jsonlToList(file_names, suppress=False)\n",
    "        df = listToDF(data, suppress=False)\n",
    "        exportDFToPickle(day, month, df, False)\n",
    "        del data, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle into DF\n",
    "def importPickle(pkl_name, suppress=True):\n",
    "    time_start = time.perf_counter()\n",
    "    df = pd.read_pickle(pkl_name)\n",
    "    time_end = time.perf_counter()\n",
    "    duration = str(round((time_end-time_start)/60,2)) + \" mins\"\n",
    "    if (suppress == False):\n",
    "        print(\"Pickle import time:\", duration)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Fact Checking Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydrates news links into JSONLs\n",
    "def newsLinksToJsonls(file_name, sheetname):\n",
    "    df = pd.read_excel(file_name, sheetname)\n",
    "    for i in range(len(df)):\n",
    "        q = df['Link'][i]\n",
    "        statuses = api.search_30_day(query=q, label='30dayenv')\n",
    "        output_name = file_name[:-5] + \"_\" + sheetname + \"_\" + \"%02d\" % i + '.jsonl'\n",
    "        with jsonlines.open(output_name, mode='w') as writer:\n",
    "            for status in statuses:\n",
    "                writer.write(status._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the Tweet root of a given erumor\n",
    "def getRumorRoots(df):\n",
    "    roots = []\n",
    "    for i in range(len(df)):\n",
    "        rootID = df['replyID'][i]\n",
    "        if (rootID == 9999999999999999999):\n",
    "            continue\n",
    "        roots.append(int(rootID))\n",
    "    roots.sort()\n",
    "    return roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the index of the root Tweet\n",
    "def getRootIndex(rootsDict, rootID):\n",
    "    for i in rootsDict.keys():\n",
    "        if (rootID in rootsDict[i]):\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets rumor roots and replies\n",
    "def getRumorRootsAndReplies(df):\n",
    "    rootsDict = {}\n",
    "    for i in range(len(df)):\n",
    "        rootID = df['replyID'][i]\n",
    "        if (rootID == 9999999999999999999):\n",
    "            continue\n",
    "        if (rootsDict.get(rootID) == None):\n",
    "            rootsDict[rootID] = [df['tweetID'][i]]\n",
    "        else:\n",
    "            replies = rootsDict.get(rootID)\n",
    "            replies.append(df['tweetID'][i])\n",
    "            rootsDict[rootID] = replies  \n",
    "    return rootsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms dataframe into dictionary\n",
    "def dfToDict(df):\n",
    "    rootsDict = {}\n",
    "    for i in range(len(df)):\n",
    "        index = str(df['index'][i])\n",
    "        if (rootsDict.get(index) == None):\n",
    "            rootsDict[index] = [int(df['tweetID'][i])]\n",
    "        else:\n",
    "            arr = rootsDict.get(index)\n",
    "            arr.append(int(df['tweetID'][i]))\n",
    "            arr = sorted(arr)\n",
    "            rootsDict[index] = arr\n",
    "    return rootsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes tweets from dictionary\n",
    "def removeTweetsFromDict(removable, rootsDict):\n",
    "    for rootID in removable:\n",
    "        rootIndex = getRootIndex(rootsDict,rootID)\n",
    "        rootsDict[rootIndex].remove(rootID)\n",
    "    return rootsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydrates dictionary into a dataframe\n",
    "def hydrateDictIntoDF(rootsDict, truth_label, simple = False):\n",
    "    data = []\n",
    "    completed_ids = []\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            arr = rootsDict[str(i)]\n",
    "        except: \n",
    "            continue\n",
    "        for j in range(len(arr)):\n",
    "            try:\n",
    "                status = api.statuses_lookup([arr[j]], tweet_mode=\"extended\")[0]._json\n",
    "                # if status is a retweet\n",
    "                try:\n",
    "                    OG_id = status['retweeted_status']['id']\n",
    "                    status = api.statuses_lookup([OG_id], tweet_mode=\"extended\")[0]._json\n",
    "                except:\n",
    "                    pass\n",
    "                # if status has already been looked at\n",
    "                if (status['id'] in completed_ids):\n",
    "                    continue\n",
    "                else:\n",
    "                    completed_ids.append(status['id'])\n",
    "            except:\n",
    "                continue\n",
    "            tweetInfo = [truth_label, i]\n",
    "            tweetInfo = tweetInfo + getTweetInfo(status)\n",
    "            data.append(tweetInfo)\n",
    "            # skip looking at reply roots if in simple mode\n",
    "            if simple:\n",
    "                continue\n",
    "            # if status is a reply\n",
    "            try:\n",
    "                while (True):\n",
    "                    root_id = status['in_reply_to_status_id']\n",
    "                    status = api.statuses_lookup([root_id], tweet_mode=\"extended\")[0]._json\n",
    "            except:\n",
    "                pass\n",
    "            # if status has already been looked at\n",
    "            if (status['id'] in completed_ids):\n",
    "                continue\n",
    "            else:\n",
    "                completed_ids.append(status['id'])\n",
    "            tweetInfo = [truth_label, i]\n",
    "            tweetInfo = tweetInfo + getTweetInfo(status)\n",
    "            data.append(tweetInfo)\n",
    "    df = listToDF(data, truth_labels=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Viral Original Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves the most viral tweets\n",
    "def getViralOriginalTweetsFromDF(RTthreshold, df):\n",
    "    df_viral = df[(df['#Retweets'] > RTthreshold) & (df['isRT'] == False)]\n",
    "    return df_viral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves the most viral tweets from a list of file names\n",
    "def getViralOriginalTweetsFromFileList(RTthreshold, file_names):\n",
    "    for i in range(len(file_names)):\n",
    "        df_day = pd.read_pickle(file_names[i])\n",
    "        if (i == 0):\n",
    "            df = getViralOriginalTweetsFromDF(RTthreshold, df_day)\n",
    "        else:\n",
    "            df = df.append(getViralOriginalTweetsFromDF(RTthreshold, df_day))\n",
    "    pklname = 'viral_OG_tweets_' + str(RTthreshold) + '.pkl'\n",
    "    df.to_pickle(pklname)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Counts Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares text query to be inputted into the counts endpoint\n",
    "def prepareCountsTextQuery(df):\n",
    "    # create regular expressions to filter out parts of the queries that are incompatible with the Twitter Counts endpoint\n",
    "    # remove links\n",
    "    http_links = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    # remove new lines\n",
    "    newlines = re.compile('\\n')\n",
    "    # remove apostrophes\n",
    "    apos = re.compile(\"[\\S]*'[\\S]*\")\n",
    "    # remove quotation marks\n",
    "    quotes = re.compile('[\\S]*(“|\"|”|’|$)[\\S]*')\n",
    "    # remove dollar signs\n",
    "    dols = re.compile('[\\S]*[$][\\S]*')\n",
    "    # remove colons\n",
    "    cols = re.compile('[:][\\S]*')\n",
    "    # remove & symbol\n",
    "    and_sym = re.compile('&amp;')\n",
    "    # remove parentheses\n",
    "    paren = re.compile('[(|)]')\n",
    "    text_query = []\n",
    "    for i in range(len(df)):\n",
    "        # filter out incompatible parts of queries using regular expressions\n",
    "        query = df['text'][i]\n",
    "        query = http_links.sub('',query)\n",
    "        query = newlines.sub(' ',query)\n",
    "        query = apos.sub('',query)\n",
    "        query = quotes.sub('',query)\n",
    "        query = dols.sub('',query)\n",
    "        query = cols.sub('',query)\n",
    "        query = and_sym.sub('',query)\n",
    "        query = paren.sub('',query)\n",
    "        # constrain the query to only search for retweets of the original author of the Tweet\n",
    "        query = 'retweets_of:' + df['user_name'][i] + ' ' + query\n",
    "        text_query.append(query)\n",
    "    return text_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares input information for the counts endpoint\n",
    "def prepareCountsInputs(df, mod_text, index, today_dt):\n",
    "    if (df['#Retweets'][index] == 0):\n",
    "        print(\"NO RETWEETS: SKIP\")\n",
    "        return None, None, None\n",
    "    else:\n",
    "        print('tweetID')\n",
    "        print(df['tweetID'][index])\n",
    "        print()\n",
    "        print(\"Original text:\")\n",
    "        print(df['text'][index])\n",
    "        print()\n",
    "        print(\"Modified text:\")\n",
    "        print(mod_text[index])\n",
    "        print()\n",
    "        print(\"Retweets:\", df['#Retweets'][index])\n",
    "        query = mod_text[index]\n",
    "        status_date = df['date'][index]\n",
    "        start_dt = datetime.strptime(status_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "        start_date = start_dt.strftime(\"%Y%m%d%H\") + '00'\n",
    "        end_dt = start_dt + timedelta(days = 31)\n",
    "        end_dt = min(end_dt, today_dt)\n",
    "        end_date = end_dt.strftime(\"%Y%m%d%H\") + '00'\n",
    "        print(\"Created at\", status_date)\n",
    "        return query, start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query counts endpoint\n",
    "def getCounts(query, start_date, end_date, bearer_token):\n",
    "    url = 'https://api.twitter.com/1.1/tweets/search/fullarchive/fullenv/counts.json'\n",
    "    headers = {'authorization': bearer_token, 'content-type': 'application/json'}\n",
    "    payload = '{\"query\":\"' + query + '\", \"fromDate\":\"' + start_date + '\", \"toDate\":\"' + end_date + '\", \"bucket\":\"hour\"}'\n",
    "    r = requests.post(url, data=payload, headers=headers)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports results to JSONL file\n",
    "def exportCountsToJsonl(r, output_name, index):\n",
    "    file_name = output_name + \"%02d\" % index + '.jsonl'\n",
    "    with jsonlines.open(file_name, mode='w') as writer:\n",
    "        writer.write(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves file names that meet a given criteria\n",
    "def retrieveFileNames(starts = '', ends = '', lengthMin=0, lengthMax=math.inf):\n",
    "    file_names = []\n",
    "    for path in Path().iterdir():\n",
    "        if path.name.startswith(starts) and path.name.endswith(ends) and len(path.name) >= lengthMin and len(path.name) <= lengthMax:\n",
    "            file_names.append(path.name)\n",
    "        file_names.sort()\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes a JSONL file containing counts output\n",
    "def processCounts(file_name):\n",
    "    f = jsonlines.open(file_name,)\n",
    "    counts = []\n",
    "    for s in f.iter():\n",
    "        for i in range(len(s['results'])):\n",
    "            counts.append(s['results'][i]['count'])\n",
    "        while (len(counts) < 744):\n",
    "            counts.append(0)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes an array of counts\n",
    "def processCountsArray(file_names):\n",
    "    counts = []\n",
    "    for file_name in file_names:\n",
    "        single_counts = processCounts(file_name)\n",
    "        counts.append(single_counts)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the structural characteristics of Tweets in a dataframe\n",
    "def getUserMentionsStats(df):\n",
    "    num_tagged_statuses = 0\n",
    "    num_tags = 0\n",
    "    for i in range(len(df)):\n",
    "        if (len(np.array(df.loc[[i]]['mentions'])[0]) > 0):\n",
    "            num_tagged_statuses += 1\n",
    "            num_tags += len(np.array(df.loc[[i]]['mentions'])[0])\n",
    "    return [num_tagged_statuses, num_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the number of statuses containing media\n",
    "def getNumStatusesWithMedia(df):\n",
    "    if ('media' in df):\n",
    "        num_statuses_w_media = 0\n",
    "        num_statuses_w_pic = 'N/A'\n",
    "        num_statuses_w_video = 'N/A'\n",
    "        for i in range(len(df)):\n",
    "            try:\n",
    "                temp = len(np.array(df.loc[[i]]['media'])[0])\n",
    "                num_statuses_w_media += 1\n",
    "            except:\n",
    "                pass\n",
    "    elif ('pic_url' and 'video_url' in df):\n",
    "        num_statuses_w_media = 0\n",
    "        num_statuses_w_pic = 0\n",
    "        num_statuses_w_video = 0\n",
    "        for i in range(len(df)):\n",
    "            if (len(df['pic_url'][i]) > 0 or type(df['video_url'][i]) == str):\n",
    "                num_statuses_w_media += 1\n",
    "            if (len(df['pic_url'][i]) > 0):\n",
    "                num_statuses_w_pic += 1\n",
    "            if (type(df['video_url'][i]) == str):\n",
    "                num_statuses_w_video += 1\n",
    "    else:\n",
    "        return 'N/A'\n",
    "    return num_statuses_w_media, num_statuses_w_pic, num_statuses_w_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets hashtag stats\n",
    "def getHashtagsStats(df):\n",
    "    num_statuses_w_hashtags = 0\n",
    "    num_hashtags = 0\n",
    "    for i in range(len(df)):\n",
    "        if (len(np.array(df.loc[[i]]['hashtags'])[0]) > 0):\n",
    "            num_statuses_w_hashtags += 1\n",
    "            num_hashtags += len(np.array(df.loc[[i]]['hashtags'])[0])\n",
    "    return [num_statuses_w_hashtags, num_hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates proportion of dataframe consisting of retweets\n",
    "def calcPropRetweet(df):\n",
    "    return len(df[df['isRT'] == True]) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets array division percentile\n",
    "def getArrayDivisionPercentile(df_col1, df_col2, percentile):\n",
    "    divisions = (df_col1/df_col2).tolist()\n",
    "    count = 0\n",
    "    while (count < len(divisions)):\n",
    "        if math.isnan(divisions[count]):\n",
    "            divisions[count] = 0\n",
    "        if math.isinf(divisions[count]):\n",
    "            del divisions[count]\n",
    "        else:\n",
    "            count += 1\n",
    "    return np.percentile(divisions,percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets engagement metrics of Tweets in a dataframe\n",
    "def getRetweetsFavsFollowersInfo(df, dataset_name):\n",
    "    n = len(df)\n",
    "    avg_RT = np.mean(df['#Retweets'])\n",
    "    avg_favs = np.mean(df['#Favorites'])\n",
    "    RTsToFavs = avg_RT/avg_favs\n",
    "    RTsToFavs_75 = getArrayDivisionPercentile(df['#Retweets'],df['#Favorites'], 75)\n",
    "    if ('#Followers' and 'verified' in df):\n",
    "        prop_RT = calcPropRetweet(df)\n",
    "        avg_fols = np.mean(df['#Followers'])\n",
    "        med_fols = np.median(df['#Followers'])\n",
    "        RTsToFols = avg_RT/avg_fols\n",
    "        RTsToFols_75 = getArrayDivisionPercentile(df['#Retweets'],df['#Followers'], 75)\n",
    "        prop_verified = sum(df['verified'])/n\n",
    "    else:\n",
    "        prop_RT = 'N/A'\n",
    "        avg_fols = 'N/A'\n",
    "        med_fols = 'N/A'\n",
    "        RTsToFols = 'N/A'\n",
    "        RTsToFols_75 = 'N/A'\n",
    "        prop_verified = 'N/A'\n",
    "    if ('#Comments' in df):\n",
    "        avg_comments = np.mean(df['#Comments'])\n",
    "    else:\n",
    "        avg_comments = 'N/A'\n",
    "    data = [[dataset_name,n,prop_RT,avg_RT,avg_favs,avg_comments,med_fols,avg_fols,RTsToFols,RTsToFols_75,RTsToFavs,RTsToFavs_75,prop_verified]]\n",
    "    result = pd.DataFrame(data, columns = ['Name', 'n','prop_RT','avg_RT','avg_favs','avg_comments','med_fols','avg_fols','RTsToFols','RTsToFols_75','RTsToFavs','RTsToFavs_75','prop_verified'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets structural characteristics of Tweets in a dataframe\n",
    "def getTagsMediaHashtagsInfo(df, dataset_name):\n",
    "    n = len(df)\n",
    "    num_statuses_w_media, num_statuses_w_pic, num_statuses_w_video = getNumStatusesWithMedia(df)\n",
    "    prop_media = num_statuses_w_media / n\n",
    "    if ('mentions' and 'hashtags' in df):\n",
    "        prop_tagged = getUserMentionsStats(df)[0] / n\n",
    "        avg_num_tags = getUserMentionsStats(df)[1] / getUserMentionsStats(df)[0]\n",
    "        prop_hashtagged = getHashtagsStats(df)[0] / n\n",
    "        avg_num_hashtags = getHashtagsStats(df)[1] / getHashtagsStats(df)[0]\n",
    "        prop_pic = 'N/A'\n",
    "        prop_video = 'N/A'\n",
    "    elif ('pic_url' and 'video_url' in df):\n",
    "        prop_tagged = 'N/A'\n",
    "        avg_num_tags = 'N/A'\n",
    "        prop_hashtagged = 'N/A'\n",
    "        avg_num_hashtags = 'N/A'\n",
    "        prop_pic = num_statuses_w_pic / n\n",
    "        prop_video = num_statuses_w_video / n\n",
    "    data = [[dataset_name, n, prop_tagged, avg_num_tags, prop_media, prop_pic, prop_video, prop_hashtagged, avg_num_hashtags]]\n",
    "    result = pd.DataFrame(data, columns = ['Name','n', 'prop_tagged', 'avg_num_tags', 'prop_media', 'prop_pic', 'prop_video', 'prop_hashtagged', 'avg_num_hashtags'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorts dictionary by value\n",
    "def sortDictByValue(dict):\n",
    "    sorted_dict = {}\n",
    "    sorted_values = sorted(dict.values(), reverse=True)\n",
    "    for i in sorted_values:\n",
    "        for key in dict.keys():\n",
    "            if dict[key] == i:\n",
    "                sorted_dict[key] = dict[key]\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts POS proportions\n",
    "def countPOSProportions(text_arr, numPOS):\n",
    "    pos = {}\n",
    "    for text in text_arr:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        for tag in tagged:\n",
    "            pos[tag[1]] = pos.get(tag[1],0) + 1\n",
    "    pos = sortDictByValue(pos)\n",
    "    pos_prop = pos.copy()\n",
    "    total = sum(pos.values())\n",
    "    for key in pos_prop.keys():\n",
    "        pos_prop[key] = round(pos_prop[key] / total,3)\n",
    "    return list(pos_prop.items())[:numPOS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets sentiment polarity\n",
    "def getSentimentPolarity(text_arr, sorted = False):\n",
    "    polarity_arr = []\n",
    "    for i in range(len(text_arr)):\n",
    "        polarity_arr.append(textblob.TextBlob(text_arr[i]).sentiment.polarity)\n",
    "    if sorted:\n",
    "        polarity_arr = sorted(polarity_arr)\n",
    "    return polarity_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets sentiment subjectivity\n",
    "def getSentimentSubjectivity(text_arr, sorted = False):\n",
    "    subjectivity_arr = []\n",
    "    for i in range(len(text_arr)):\n",
    "        subjectivity_arr.append(textblob.TextBlob(text_arr[i]).sentiment.subjectivity)\n",
    "    if sorted:\n",
    "        subjectivity_arr = sorted(subjectivity_arr)\n",
    "    return subjectivity_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets array percentiel\n",
    "def getArrPercentile(arr, percentile):\n",
    "    return sorted(arr)[int(percentile*len(arr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs sentiment analysis\n",
    "def sentimentAnalysis(text_arr, dataset_name):\n",
    "    polarity = getSentimentPolarity(text_arr)\n",
    "    avg_polarity = np.mean(polarity)\n",
    "    std_polarity = np.std(polarity)\n",
    "    fifth_perc_polarity = getArrPercentile(polarity, 0.05)\n",
    "    ninetyfifth_perc_polarity = getArrPercentile(polarity, 0.95)\n",
    "    avg_subjectivity = np.mean(getSentimentSubjectivity(text_arr))\n",
    "    data = [[dataset_name, avg_polarity, std_polarity, fifth_perc_polarity, ninetyfifth_perc_polarity, avg_subjectivity]]\n",
    "    df = pd.DataFrame(data, columns = ['Name', 'Avg polarity', 'Std polarity', '5th perc. polarity', '95th perc. polarity', 'Avg subjectivity'])\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
